{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Maximum Lyapunov Exponent via autograd\n",
    "\n",
    "21.07.2025: Use this file as blueprint to update others\n",
    "\n",
    "Genrating videos to better understand the dynamics via mFTLEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import importlib\n",
    "\n",
    "# Juptyer magic: For export. Makes the plots size right for the screen \n",
    "%matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "%config InlineBackend.figure_formats = ['svg'] \n",
    "\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "seed = np.random.randint(1,200)\n",
    "# seed = 61 #59\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "print(seed)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = False #this leads to squared loss in the training\n",
    "output_dim = 1 #for model architecture later, but need it already for dataloader\n",
    "data_noise = 0.05\n",
    "num_points = 5000\n",
    "plotlim = [-3, 3]\n",
    "subfolder = 'MLE_4params_less_noise' #all the files generated from this notebook get saved into this subfolder \n",
    "\n",
    "import os\n",
    "if not os.path.exists(subfolder):\n",
    "    os.makedirs(subfolder)\n",
    "\n",
    "\n",
    "label = 'scalar' if (cross_entropy or output_dim == 1) else 'vector' #MSE allows both scalar and vector output, cross_entropy only allows \"scalar\" output here\n",
    "print('label:', label)\n",
    "\n",
    "from models.training import create_dataloader\n",
    "dataloader, dataloader_viz = create_dataloader('moons', noise = data_noise, num_points = num_points, plotlim = plotlim, random_state = seed, label = label, filename = subfolder + '/trainingset')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import of the model dynamics that describe the neural ODE\n",
    "#The dynamics are based on the torchdiffeq package, that implements ODE solvers in the pytorch setting \n",
    "from models.neural_odes import NeuralODEvar\n",
    "\n",
    "#for neural ODE based networks the network width is constant. In this example the input is 2 dimensional\n",
    "hidden_dim, data_dim = 2, 2 \n",
    "augment_dim = 0\n",
    "\n",
    "#T is the end time of the neural ODE evolution, time_steps are the amount of discretization steps for the ODE solver\n",
    "T, time_steps = 10, 100 #\n",
    "step_size = T/time_steps\n",
    "num_params = 5 #the number of distinct parameters present in the interval. they are spread equidistant over the interval [0, T]. As there are 100 time_steps, the interval is divided into 10 parts, each of length 1 with 10 time_steps per subinterval.\n",
    "bound = 0.\n",
    "turnpike = False\n",
    "l2_factor = 0.1 #for the regularization term in the loss function, that penalizes large weights\n",
    "\n",
    "non_linearity = 'relu' #'relu' #\n",
    "architecture = 'inside' #outside\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and generating level sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 100 #number of optimization runs in which the dataset is used for gradient decent\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "anode = NeuralODEvar(device, data_dim, hidden_dim, output_dim=output_dim, augment_dim=augment_dim, non_linearity=non_linearity, \n",
    "                    architecture=architecture, T=T, time_steps=time_steps, num_params = num_params, cross_entropy=cross_entropy)\n",
    "\n",
    "\n",
    "optimizer_anode = torch.optim.Adam(anode.parameters(), lr=1e-3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.training import doublebackTrainer\n",
    "\n",
    "trainer_anode = double_c__/adsfasdf_/fasdfdasfxorasdfasdfasdfbackTrainer(anode, optimizer_anode, device, cross_entropy=cross_entropy, turnpike = turnpike,\n",
    "                         bound=bound, l2_factor=0.01, verbose = True) \n",
    "trainer_anode.train(dataloader, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plots.plots\n",
    "importlib.reload(plots.plots)\n",
    "from plots.plots import classification_levelsets\n",
    "\n",
    "footnote = f'{num_epochs = }, {cross_entropy = }, {num_params = }, {l2_factor = },\\n {time_steps = }, {output_dim = }, {data_noise = }, {seed = }'\n",
    "        \n",
    "fig_name_base = os.path.join(subfolder, 'levelsets')\n",
    "classification_levelsets(anode, fig_name_base, footnote = footnote)\n",
    "from IPython.display import Image\n",
    "img1 = Image(filename = fig_name_base + '.png', width = 500)\n",
    "display(img1)\n",
    "\n",
    "plt.plot(trainer_anode.histories['epoch_loss_history'])\n",
    "plt.xlim(0, len(trainer_anode.histories['epoch_loss_history']) - 1)\n",
    "plt.ylim(0)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import plots.gifs\n",
    "importlib.reload(plots.gifs) # Reload the module to ensure the latest changes are applied\n",
    "from plots.gifs import trajectory_gif\n",
    "from IPython.display import Image\n",
    "\n",
    "#the function trajectory_gif creates the evolution trajectories of the input data through the network\n",
    "#the passing time corresponds to the depth of the network\n",
    "\n",
    "for X_viz, y_viz in dataloader_viz:\n",
    "    trajectory_gif(anode, X_viz[0:50], y_viz[0:50], timesteps=time_steps, filename = subfolder + '/trajectory', axlim = 8, dpi = 100)\n",
    "    break\n",
    "\n",
    "#Display of the generated gif\n",
    "traj = Image(filename=subfolder + \"/trajectory.gif\")\n",
    "display(traj)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lyapunov exponent computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FTLE import LEs\n",
    "\n",
    "#Example of how to use the LEs function to compute Lyapunov exponents\n",
    "# Define inputs\n",
    "input1 = torch.tensor([1, 0], dtype=torch.float32)\n",
    "time_interval = torch.tensor([0, T], dtype=torch.float32)\n",
    "\n",
    "les = LEs(input1, anode, time_interval=time_interval) #computes the Lyapunov exponents (l1>=l2>=...l_min) for the input1 over the time interval\n",
    "print(les)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot ANODE FTLEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FTLE import LE_grid\n",
    "\n",
    "output_max, output_min = LE_grid(anode,x_amount = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing grid plots\n",
    "importlib.reload(plots.plots) # Reload the module to ensure the latest changes are applied\n",
    "from plots.plots import plot_trajectory\n",
    "\n",
    "ax_lim = 2\n",
    "stepsize = T/time_steps\n",
    "interval = torch.tensor([0., T], dtype=torch.float32)\n",
    "\n",
    "\n",
    "anodeimg_max = plt.imshow(np.rot90(output_max), origin='upper', extent=(-ax_lim, ax_lim, -ax_lim, ax_lim),cmap = 'viridis')\n",
    "vmin_max, vmax_max = anodeimg_max.get_clim()\n",
    "\n",
    "plt.colorbar()  # Show color scale\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anodeimg_max = plt.imshow(np.rot90(output_max), origin='upper', extent=(-2, 2, -2, 2),cmap = 'viridis')\n",
    "vmin_max, vmax_max = anodeimg_max.get_clim()\n",
    "\n",
    "plt.colorbar()  # Show color scale\n",
    "plt.savefig(subfolder + '/MLE_max.png',bbox_inches='tight', dpi=300, format='png', facecolor = 'white')\n",
    "plt.close()\n",
    "\n",
    "anodeimg_min = plt.imshow(np.rot90(output_min), origin='upper', extent=(-2, 2, -2, 2),cmap = 'viridis')#, norm=CenteredNorm(vcenter=0)) # cmap='viridis')\n",
    "vmin_min, vmax_min = anodeimg_min.get_clim()\n",
    "\n",
    "plt.colorbar()  # Show color scale\n",
    "plt.savefig(subfolder + '/MLE_min.png',bbox_inches='tight', dpi=300, format='png', facecolor = 'white')\n",
    "plt.close()\n",
    "\n",
    "img1 = Image(filename = fig_name_base + '.png', width = 400)\n",
    "img2 = Image(filename = subfolder + '/MLE_max.png', width = 400)\n",
    "img3 = Image(filename = subfolder + '/MLE_min.png', width = 400)\n",
    "\n",
    "display(img1,img2,img3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video 1: FTLE for each subinterval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(plots.plots)\n",
    "\n",
    "import os\n",
    "\n",
    "from plots.plots import plot_points_from_traj, input_to_traj_and_color, plot_vectorfield\n",
    "from plots.plots import create_gif_subintervals\n",
    "import imageio\n",
    "import io\n",
    "\n",
    "filename = subfolder + '/FTLE_per_param'\n",
    "create_gif_subintervals(anode, le_density=40, filename=filename)\n",
    "LEevo_test = Image(filename=filename + \".gif\")\n",
    "display(LEevo_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the autonomous case any interval of same length should give the same LE. So the above gif makes sense!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_gif_shrinkingintervals(model, le_density = 30, point_density = 20, filename = 'LE_shrinking'):\n",
    "#     \"\"\"\n",
    "#     Creates a GIF showing the evolution of input points and the future Lyapunov exponents. As the time progresses, the remaining Lyapunov exponent intervals shrink, reflecting the model's dynamics.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - model: The neural ODE model.\n",
    "#     - le_amount: Number of Lyapunov exponent intervals.\n",
    "#     - vf_amount: Number of vector field points.\n",
    "#     - filename: Base name for the output files.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     #density of FTLE grid\n",
    "\n",
    "\n",
    "#     ###point plot preparation####\n",
    "#     # Define the grid for vector field visualization\n",
    "#     x = torch.linspace(-2,2,point_density)\n",
    "#     y = torch.linspace(-2,2,point_density)\n",
    "#     X, Y = torch.meshgrid(x, y)\n",
    "#     inputs_grid = torch.stack([X,Y], dim=-1)\n",
    "#     inputs_grid = inputs_grid.view(-1,2) #to be able to input all the grid values into the model at once\n",
    "\n",
    "#     #compute traj and colors of grid inputs first, later no more evaluations of model needed\n",
    "#     trajs, colors = input_to_traj_and_color(model, inputs_grid)\n",
    "\n",
    "    \n",
    "#     T = model.T\n",
    "#     step_size = T/model.time_steps #time step for the integration\n",
    "#     eps = 0.1 * step_size #this should make sure we stay inside an interval with constant parameters for the integration\n",
    "#     t_values = np.arange(0 + eps, T-step_size, step_size) #all discretization steps computed in the nODE flow\n",
    "    \n",
    "#     images = []\n",
    "    \n",
    "#     for t in t_values:\n",
    "    \n",
    "#         ###FTLE plot\n",
    "#         le_interval = torch.tensor([t, T], dtype=torch.float32)\n",
    "\n",
    "#         output_max, _ = LE_grid(model, le_density, le_interval)\n",
    "#         plt.imshow(np.rot90(output_max), origin='upper', extent=(-2, 2, -2, 2), cmap='viridis')\n",
    "#         cbar = plt.colorbar()\n",
    "#         cbar.ax.tick_params(labelsize=10)  # Adjust tick label size\n",
    "        \n",
    "#         ### Plot points from trajectory\n",
    "#         plot_points_from_traj(trajs, colors, model, t) #this + step_size here does not make sense yet must still be explained.\n",
    "        \n",
    "#         plt.gca().set_aspect('equal', adjustable='box')  # more robust than plt.axis('equal')\n",
    "#         plt.xlim(-2, 2)\n",
    "#         plt.ylim(-2, 2)\n",
    "#         plt.xlabel(r\"$x_1$\", fontsize=5)\n",
    "#         plt.ylabel(r\"$x_2$\", fontsize=5)\n",
    "#         plt.tick_params(axis='both', which='major', labelsize=5)\n",
    "#         plt.title(f'Time {t:.2f}, FTLE interval [{le_interval[0].item():.2f}, {le_interval[1].item():.2f}] ', fontsize = 7)\n",
    "        \n",
    "#         buf = io.BytesIO()\n",
    "#         plt.savefig(buf, bbox_inches='tight', dpi=200, format='png', facecolor='white')\n",
    "#         buf.seek(0)  # rewind to beginning of buffer\n",
    "#         images.append(imageio.imread(buf))  # or use PIL.Image.open(buf)\n",
    "#         buf.close()\n",
    "#         plt.close()\n",
    "#         print(f'Saved plot for t={t:.1f}')\n",
    "    \n",
    "#     imageio.mimsave(filename + '.gif', images, fps=2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video 2: FTLE for shrinking interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(plots.plots) # Reload the module to ensure the latest changes are applied\n",
    "from plots.plots import create_gif_shrinkingintervals\n",
    "\n",
    "\n",
    "create_gif_shrinkingintervals(anode, filename=subfolder + '/shrinking_LE')\n",
    "LEevo_test = Image(filename=subfolder + '/shrinking_LE.gif')\n",
    "display(LEevo_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralODE",
   "language": "python",
   "name": "neuralode"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
