{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File to build up augmented training via manipulating FTLEs during training\n",
    "here standard training and FTLE suppression training is compared\n",
    "\n",
    "to continue an example from MLE_master make sure to use the identical random seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import importlib\n",
    "from plots.plots import set_plot_defaults\n",
    "set_plot_defaults() # set the plot default formatting\n",
    "\n",
    "# Juptyer magic: For export. Makes the plots size right for the screen \n",
    "%matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "%config InlineBackend.figure_formats = ['svg'] \n",
    "\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "seed = np.random.randint(1,200)\n",
    "seed = 60 #59\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "print(seed)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = False #this leads to squared loss in the training\n",
    "output_dim = 2 #for model architecture later, but need it already for dataloader\n",
    "data_noise = 0.05\n",
    "num_points = 5000\n",
    "plotlim = [-2, 2]\n",
    "subfolder = 'LE_training' #all the files generated from this notebook get saved into this subfolder\n",
    "\n",
    "import os\n",
    "if not os.path.exists(subfolder):\n",
    "    os.makedirs(subfolder)\n",
    "\n",
    "\n",
    "label = 'vector' #MSE allows both scalar and vector output, cross_entropy only allows \"scalar\" output here\n",
    "\n",
    "\n",
    "from models.training import create_dataloader\n",
    "dataloader, dataloader_viz = create_dataloader('moons', noise = data_noise, cross_entropy = cross_entropy, num_points = num_points, plotlim = plotlim, random_state = seed, label = label, filename = subfolder + '/trainingset')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import of the model dynamics that describe the neural ODE\n",
    "#The dynamics are based on the torchdiffeq package, that implements ODE solvers in the pytorch setting\n",
    "from models.neural_odes import NeuralODEvar\n",
    "\n",
    "#for neural ODE based networks the network width is constant. In this example the input is 2 dimensional\n",
    "hidden_dim, data_dim = 5, 2 \n",
    "\n",
    "#T is the end time of the neural ODE evolution, time_steps are the amount of discretization steps for the ODE solver\n",
    "T, time_steps = 10, 100 #\n",
    "step_size = T/time_steps\n",
    "num_params = 1 #the number of distinct parameters present in the interval. they are spread equidistant over the interval [0, T]. As there are 100 time_steps, the interval is divided into 10 parts, each of length 1 with 10 time_steps per subinterval.\n",
    "layers_hidden = 1 #the amount of hidden layers in the vector field (these layers do not correspond to time discretization but to the expressivity of the vector field)\n",
    "\n",
    "non_linearity = 'tanh' #'relu'\n",
    "architecture = 'inside' #outside\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and generating level sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 60 #number of optimization runs in which the dataset is used for gradient decent\n",
    "\n",
    "#####LE training parameters\n",
    "le_reg = 10.\n",
    "time_interval = torch.tensor([0., 2* T/5]) #compute LE only on the early subinterval\n",
    "le_threshold = 0.05\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "anode_control = NeuralODEvar(device, data_dim, hidden_dim, output_dim = output_dim, non_linearity=non_linearity, \n",
    "                    architecture=architecture, T=T, time_steps=time_steps, num_params = num_params, layers_hidden = layers_hidden, cross_entropy=cross_entropy)\n",
    "\n",
    "\n",
    "optimizer_anode_control = torch.optim.Adam(anode_control.parameters(), lr=1e-3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "anode = NeuralODEvar(device, data_dim, hidden_dim, output_dim = output_dim, non_linearity=non_linearity, \n",
    "                    architecture=architecture, T=T, time_steps=time_steps, num_params = num_params, layers_hidden = layers_hidden, cross_entropy=cross_entropy)\n",
    "\n",
    "\n",
    "optimizer_anode = torch.optim.Adam(anode.parameters(), lr=1e-3) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numpy import mean\n",
    "import torch.nn as nn\n",
    "\n",
    "from FTLE import input_to_output\n",
    "from FTLE import LEs\n",
    "import torch.nn.functional as F\n",
    "from plots.plots import plot_FTLEs\n",
    "\n",
    "import models.training\n",
    "importlib.reload(models.training) # Reload the module to ensure the latest changes are applied\n",
    "\n",
    "from models.training import FTLE_Trainer\n",
    "\n",
    "\n",
    "losses = {'mse': nn.MSELoss(), \n",
    "          'cross_entropy': nn.CrossEntropyLoss(), \n",
    "          'ell1': nn.SmoothL1Loss()\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.training import doublebackTrainer\n",
    "\n",
    "\n",
    "\n",
    "trainer_anode_control = doublebackTrainer(anode_control, optimizer_anode_control, device, cross_entropy=cross_entropy, turnpike = False,\n",
    "                        verbose = True) \n",
    "trainer_anode_control.train(dataloader, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trainer_anode = FTLE_Trainer(anode, device = device, le_reg = le_reg, time_interval = time_interval, le_threshold = le_threshold) \n",
    "trainer_anode.train(dataloader, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FTLE import LEs\n",
    "\n",
    "x_batch, y_batch = next(iter(dataloader_viz))\n",
    "print(x_batch[0:5])\n",
    "input = torch.tensor([0,1.])\n",
    "le_summed = torch.zeros(0)\n",
    "for input in x_batch:\n",
    "    print('input ', input)\n",
    "    le = LEs(input,anode)\n",
    "    print('les ', le)\n",
    "    print('le_max ', le[0])\n",
    "    le_summed += abs(le[0])\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import plots.plots\n",
    "importlib.reload(plots.plots) # Reload the module to ensure the latest changes are applied\n",
    "from plots.plots import classification_levelsets\n",
    "from IPython.display import Image, HTML\n",
    "import time\n",
    "\n",
    "footnote = f'{le_reg = }, le_time_interval = {trainer_anode.time_interval}, le_threshold = {trainer_anode.le_threshold} {num_epochs = }, {cross_entropy = }, \\n {num_params = },{time_steps = }, {output_dim = }, {hidden_dim = }, {data_noise = }, {seed = }'\n",
    "footnote_control = f'le_reg = 0, {num_epochs = }, {cross_entropy = }, {num_params = },\\n {time_steps = }, {output_dim = }, {hidden_dim = }, {data_noise = }, {seed = }'\n",
    "\n",
    "num_levels = 6\n",
    "        \n",
    "fig_name_base = os.path.join(subfolder, 'levelsets')\n",
    "classification_levelsets(anode, fig_name_base, footnote = footnote, num_levels = num_levels)\n",
    "fig_name_base_control = os.path.join(subfolder, 'levelsets_control')\n",
    "classification_levelsets(anode_control, fig_name_base_control, footnote = footnote_control,  num_levels = num_levels)\n",
    "\n",
    "\n",
    "#If i do not add the time.time() to the image url, the image is not reloaded in the notebook because of caching\n",
    "display(HTML(f\"\"\"\n",
    "<div style=\"display:flex; gap:10px;\">\n",
    "    <img src=\"{fig_name_base + '.png'}?{time.time()}\" width=\"400\">\n",
    "    <img src=\"{fig_name_base_control + '.png'}?{time.time()}\" width=\"400\">\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "plt.plot(trainer_anode.histories['epoch_loss_history'], label = 'loss_ges')\n",
    "plt.xlim(0, len(trainer_anode.histories['epoch_loss_history']) - 1)\n",
    "plt.ylim(0)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "# plt.show()\n",
    "\n",
    "plt.plot(trainer_anode.histories['epoch_loss_le_history'], label = 'loss_le')\n",
    "plt.plot(trainer_anode_control.histories['epoch_loss_history'], label = 'loss_control')\n",
    "plt.legend()\n",
    "plt.savefig(subfolder + '/losses.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plots.plots\n",
    "importlib.reload(plots.plots) # Reload the module to ensure the latest changes are applied\n",
    "from plots.plots import trajectory_gif_new\n",
    "\n",
    "\n",
    "for X_viz, y_viz in dataloader_viz:\n",
    "    trajectory_gif_new(anode, X_viz[0:50], y_viz[0:50], timesteps=time_steps, filename = subfolder + '/trajectory_new.gif', axlim = 8, dpi = 100)\n",
    "    break\n",
    "\n",
    "\n",
    "\n",
    "for X_viz, y_viz in dataloader_viz:\n",
    "    trajectory_gif_new(anode_control, X_viz[0:50], y_viz[0:50], timesteps=time_steps, filename = subfolder + '/trajectory_new_control.gif', axlim = 8, dpi = 100)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(HTML(f\"\"\"\n",
    "<div style=\"display:flex; gap:10px;\">\n",
    "    <img src=\"{subfolder + \"/trajectory_new\" + str(time_steps - 1) + \".png\"}?{time.time()}\" width=\"400\">\n",
    "    <img src=\"{subfolder + \"/trajectory_new_control\" + str(time_steps - 1) + \".png\"}?{time.time()}\" width=\"400\">\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "traj = Image(filename=subfolder + \"/trajectory_new.gif\")\n",
    "display(traj)\n",
    "\n",
    "traj = Image(filename=subfolder + \"/trajectory_new_control.gif\")\n",
    "display(traj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FTLE import LEs\n",
    "\n",
    "#Example of how to use the LEs function to compute Lyapunov exponents\n",
    "# Define inputs\n",
    "input1 = torch.tensor([1, 0], dtype=torch.float32)\n",
    "time_interval = torch.tensor([0, T], dtype=torch.float32)\n",
    "\n",
    "les = LEs(input1, anode, time_interval=time_interval) #computes the Lyapunov exponents (l1>=l2>=...l_min) for the input1 over the time interval\n",
    "print(les)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot ANODE FTLEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FTLE import LE_grid\n",
    "le_density = 100 #the amount of points in the grid for the Lyapunov exponent computation\n",
    "output_max, output_min = LE_grid(anode,x_amount = le_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FTLE import LE_grid\n",
    "output_max_control, output_min_control = LE_grid(anode_control,x_amount = le_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(plots.plots) # Reload the module to ensure the latest changes are applied\n",
    "from plots.plots import plot_FTLEs\n",
    "\n",
    "plot_FTLEs(output_max, filename = subfolder + '/MLE_max')\n",
    "plot_FTLEs(output_max_control, filename = subfolder + '/MLE_max_control')\n",
    "\n",
    "mFTLEs = Image(filename=subfolder + '/MLE_max.png', width = 400)\n",
    "display(mFTLEs)\n",
    "\n",
    "mFTLEs_control = Image(filename=subfolder + '/MLE_max_control.png', width = 400)\n",
    "display(mFTLEs_control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evolution of the Finite Time Lyapunov Exponents over the whole interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video 1: FTLE for each subinterval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(plots.plots)\n",
    "\n",
    "import os\n",
    "\n",
    "from plots.plots import plot_points_from_traj, input_to_traj_and_color, plot_vectorfield\n",
    "from plots.plots import create_gif_subintervals\n",
    "import imageio\n",
    "import io\n",
    "\n",
    "le_density = 50\n",
    "save_pngs = True #whether to save the individual png files used to create the gifs\n",
    "\n",
    "filename = subfolder + '/FTLE_per_param'\n",
    "create_gif_subintervals(anode, le_density=le_density, filename=filename, save_pngs=save_pngs)\n",
    "LEevo_test = Image(filename=filename + \".gif\")\n",
    "display(LEevo_test)\n",
    "\n",
    "filename = subfolder + '/FTLE_per_param_control'\n",
    "create_gif_subintervals(anode_control, le_density=le_density, filename=filename, save_pngs=save_pngs)\n",
    "LEevo_test_control = Image(filename=filename + \".gif\")\n",
    "display(LEevo_test_control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the autonomous case any interval of same length should give the same LE. So the above gif makes sense!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video 2: FTLE for shrinking interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(plots.plots) # Reload the module to ensure the latest changes are applied\n",
    "from plots.plots import create_gif_shrinkingintervals\n",
    "\n",
    "\n",
    "create_gif_shrinkingintervals(anode, le_density = le_density, filename=subfolder + '/shrinking_LE', save_pngs=save_pngs)\n",
    "LEevo_test = Image(filename=subfolder + '/shrinking_LE.gif')\n",
    "display(LEevo_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_gif_shrinkingintervals(anode_control, le_density = le_density, filename=subfolder + '/shrinking_LE_control', save_pngs=save_pngs)\n",
    "LEevo_control = Image(filename=subfolder + '/shrinking_LE_control.gif')\n",
    "display(LEevo_control)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralODE",
   "language": "python",
   "name": "neuralode"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
